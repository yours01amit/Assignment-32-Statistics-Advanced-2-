{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13da8ba2",
   "metadata": {},
   "source": [
    "# Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example.\n",
    "\n",
    "Ans: The Probability Mass Function (PMF) and Probability Density Function (PDF) are two fundamental concepts in probability and statistics used to describe the probability distribution of a random variable.\n",
    "\n",
    "* Probability Mass Function (PMF):\n",
    "The PMF is applicable to discrete random variables. It gives the probability that a discrete random variable takes on a specific value. In other words, it associates each value of the random variable with its corresponding probability of occurrence.\n",
    "Mathematically, for a discrete random variable X, the PMF is denoted by P(X = x), where 'x' is a specific value that X can take. The PMF satisfies two properties:\n",
    "\n",
    "P(X = x) ≥ 0 for all possible values of X.\n",
    "Σ P(X = x) over all possible values of X = 1.\n",
    "Example of PMF:\n",
    "Let's consider the random variable X representing the outcome of rolling a fair six-sided die. The possible values of X are {1, 2, 3, 4, 5, 6}. Since each outcome is equally likely for a fair die, the PMF for X will be:\n",
    "\n",
    "P(X = 1) = 1/6\n",
    "P(X = 2) = 1/6\n",
    "P(X = 3) = 1/6\n",
    "P(X = 4) = 1/6\n",
    "P(X = 5) = 1/6\n",
    "P(X = 6) = 1/6\n",
    "\n",
    "* Probability Density Function (PDF):\n",
    "The PDF is applicable to continuous random variables. It represents the likelihood of a continuous random variable falling within a particular range of values. Unlike the PMF, which gives the exact probability of individual values, the PDF only provides probabilities over intervals.\n",
    "Mathematically, for a continuous random variable X, the PDF is denoted by f(X), and it satisfies two properties:\n",
    "\n",
    "f(X) ≥ 0 for all values of X.\n",
    "The total area under the curve of the PDF over the entire range of X equals 1.\n",
    "Example of PDF:\n",
    "Let's consider a continuous random variable Y representing the height of people in a certain population. The PDF for Y might follow a normal distribution with a mean of 170 cm and a standard deviation of 5 cm.\n",
    "\n",
    "The PDF is represented by a probability density curve. The probability of a person having a height within a certain range can be determined by finding the area under the curve over that range.\n",
    "\n",
    "For example, the probability of a person having a height between 165 cm and 175 cm can be calculated by finding the area under the curve of the PDF between these two values.\n",
    "\n",
    "Please note that in continuous distributions, the probability of any specific value is zero since the number of possible values is infinite. Instead, we calculate the probability of intervals or ranges of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d26b9",
   "metadata": {},
   "source": [
    "# Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?\n",
    "\n",
    "Ans: Cumulative Density Function (CDF) is a concept used in probability and statistics to describe the distribution of a random variable. The CDF gives the probability that the random variable will take a value less than or equal to a specific value. In other words, it provides the cumulative probability of the random variable being less than or equal to a given value.\n",
    "\n",
    "Mathematically, for a random variable X, the CDF is denoted by F(x) and is defined as:\n",
    "\n",
    "F(x) = P(X ≤ x)\n",
    "\n",
    "Where:\n",
    "\n",
    "F(x) is the cumulative probability of X being less than or equal to x.\n",
    "P(X ≤ x) is the probability that the random variable X takes a value less than or equal to x.\n",
    "Here's an example to illustrate the concept:\n",
    "\n",
    "Let's consider a random variable X representing the result of rolling a fair six-sided die. The possible outcomes of X are 1, 2, 3, 4, 5, and 6, each with equal probability of 1/6.\n",
    "\n",
    "The CDF of X would look like this:\n",
    "\n",
    "* F(1) = P(X ≤ 1) = P(X = 1) = 1/6\n",
    "* F(2) = P(X ≤ 2) = P(X = 1 or X = 2) = 2/6 = 1/3\n",
    "* F(3) = P(X ≤ 3) = P(X = 1 or X = 2 or X = 3) = 3/6 = 1/2\n",
    "* F(4) = P(X ≤ 4) = P(X = 1 or X = 2 or X = 3 or X = 4) = 4/6 = 2/3\n",
    "* F(5) = P(X ≤ 5) = P(X = 1 or X = 2 or X = 3 or X = 4 or X = 5) = 5/6\n",
    "* F(6) = P(X ≤ 6) = P(X = 1 or X = 2 or X = 3 or X = 4 or X = 5 or X = 6) = 1\n",
    "The CDF provides a complete picture of the probabilities associated with the random variable. It helps in various statistical analyses and calculations. Some reasons why CDF is useful include:\n",
    "\n",
    "Probability calculations: The CDF allows us to quickly determine the probability that a random variable falls within a certain range.\n",
    "\n",
    "Understanding the distribution: By visualizing the CDF, we can get insights into the overall behavior of the random variable, such as how likely it is to take on specific values.\n",
    "\n",
    "Comparison of distributions: CDFs can be used to compare different probability distributions, which can be valuable in decision-making and modeling.\n",
    "\n",
    "Percentile calculations: CDFs allow us to find percentiles easily. For example, the 75th percentile corresponds to the value of x for which F(x) = 0.75.\n",
    "\n",
    "Overall, the Cumulative Density Function is a fundamental concept in probability theory and statistics, helping us understand and analyze random variables and their distributions more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77eb0c7",
   "metadata": {},
   "source": [
    "# Q3: What are some examples of situations where the normal distribution might be used as a model? Explain how the parameters of the normal distribution relate to the shape of the distribution.\n",
    "\n",
    "Ans: The normal distribution, also known as the Gaussian distribution, is a fundamental probability distribution used in various fields due to its versatile nature and widespread applicability. It is characterized by its bell-shaped curve and is fully described by two parameters: the mean (μ) and the standard deviation (σ). Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "* Natural Phenomena: Many natural phenomena follow a normal distribution. For instance, the heights of a population, the weights of objects, and the measurement errors in scientific experiments are often modeled using the normal distribution.\n",
    "\n",
    "* Financial Markets: In finance, stock returns, asset prices, and other financial variables often exhibit a roughly normal distribution, making the normal distribution a common choice for modeling these data.\n",
    "\n",
    "* IQ Scores: Intelligence quotient (IQ) scores of a large population tend to follow a normal distribution, with a mean of 100 and a standard deviation of 15.\n",
    "\n",
    "* Test Scores: Standardized test scores, like SAT or GRE scores, often approximate a normal distribution, allowing educators and policymakers to analyze and compare results effectively.\n",
    "\n",
    "* Biological Measurements: Biological measurements such as blood pressure, cholesterol levels, and reaction times are often modeled using the normal distribution.\n",
    "\n",
    "* Measurement Errors: In various scientific experiments and measurements, errors can be approximated using a normal distribution.\n",
    "\n",
    "Parameters of the normal distribution and their relation to the shape of the distribution:\n",
    "\n",
    "* Mean (μ): The mean represents the center of the distribution. It determines where the peak of the bell-shaped curve is located. Shifting the mean to the left or right will move the entire distribution accordingly without changing its shape.\n",
    "\n",
    "* Standard Deviation (σ): The standard deviation measures the spread or dispersion of the data points around the mean. A smaller standard deviation results in a narrower and taller bell-shaped curve, while a larger standard deviation leads to a wider and flatter curve.\n",
    "\n",
    "Together, the mean and standard deviation fully characterize the normal distribution. The probability density function (PDF) of the normal distribution is given by:\n",
    "\n",
    "f(x) = (1 / (σ√(2π))) * exp(-(x - μ)² / (2σ²))\n",
    "\n",
    "where:\n",
    "\n",
    "* x is the value of the random variable.\n",
    "* μ is the mean of the distribution, which determines the center.\n",
    "* σ is the standard deviation, which controls the spread of the distribution.\n",
    "* π is the mathematical constant pi (approximately 3.14159).\n",
    "* exp(z) represents the exponential function e^z.\n",
    "The normal distribution is symmetrical around its mean, and about 68% of the data falls within one standard deviation of the mean (between μ - σ and μ + σ), while approximately 95% of the data falls within two standard deviations of the mean (between μ - 2σ and μ + 2σ). This property, along with the Central Limit Theorem, makes the normal distribution an essential tool in statistical inference and hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade9007",
   "metadata": {},
   "source": [
    "# Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution.\n",
    "\n",
    "Ans: The normal distribution holds significant importance in various fields due to its many desirable properties and widespread occurrence in real-life phenomena. Some of the key reasons for its importance are:\n",
    "\n",
    "* Central Limit Theorem: One of the most crucial aspects of the normal distribution is the Central Limit Theorem (CLT). According to the CLT, the sum or average of a large number of independent and identically distributed random variables will be approximately normally distributed, regardless of the distribution of the original variables. This property allows statisticians and data analysts to make reliable inferences about population parameters from sample data.\n",
    "\n",
    "* Data Modeling: Many natural processes and human characteristics tend to follow the normal distribution. It serves as a useful model for representing real-world data, enabling researchers and analysts to understand, describe, and predict outcomes accurately.\n",
    "\n",
    "* Statistical Inference: The normal distribution simplifies many statistical calculations and tests, making hypothesis testing, confidence intervals, and other inference techniques more manageable and interpretable.\n",
    "\n",
    "* Bell Curve: The bell-shaped curve of the normal distribution is intuitive and easy to visualize. It helps in understanding the probability of events occurring within specific ranges, making it suitable for decision-making and risk assessment.\n",
    "\n",
    "Real-life examples of the normal distribution include:\n",
    "\n",
    "* Heights of Individuals: In a large population, adult human heights often follow a normal distribution. The mean height tends to be around the center of the curve, and heights deviate from the mean symmetrically in both directions.\n",
    "\n",
    "* Exam Scores: In educational settings, exam scores for large classes of students often approximate a normal distribution. If the exam is well-designed and the students' abilities are diverse, the scores will cluster around the average, following the bell-shaped curve.\n",
    "\n",
    "* IQ Scores: Intelligence quotient (IQ) scores are standardized to have a normal distribution with a mean of 100 and a standard deviation of 15. This allows for easy comparison and analysis of intelligence levels in a population.\n",
    "\n",
    "* Body Measurements: Various body measurements, such as weight, waist circumference, and blood pressure, are often modeled using the normal distribution. These measurements tend to cluster around a central value, with variations represented by the standard deviation.\n",
    "\n",
    "* Measurement Errors: In scientific experiments and measurements, errors often follow a normal distribution. Researchers use this property to estimate the uncertainty associated with their measurements and to make predictions about the true values.\n",
    "\n",
    "* Stock Market Returns: Daily or monthly returns of financial assets in the stock market can often be modeled using a normal distribution, although in practice, financial data might have fatter tails and exhibit more extreme events (e.g., market crashes) than what a strict normal distribution would predict.\n",
    "\n",
    "Overall, the normal distribution is a powerful and versatile tool that aids in understanding various aspects of real-world phenomena, making it a cornerstone of statistics, probability theory, and data analysis.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e956b36",
   "metadata": {},
   "source": [
    "# Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?\n",
    "\n",
    "Ans: The Bernoulli distribution is a fundamental probability distribution that represents a random experiment with two possible outcomes: success (usually denoted as 1) and failure (usually denoted as 0). It's named after Jacob Bernoulli, a Swiss mathematician. The distribution is characterized by a single parameter, often denoted as \"p,\" which represents the probability of success.\n",
    "\n",
    "Mathematically, the probability mass function (PMF) of the Bernoulli distribution is given by:\n",
    "\n",
    "P(X = x) = p^x * (1 - p)^(1 - x)\n",
    "\n",
    "Where:\n",
    "\n",
    "* X is the random variable that takes values 0 or 1.\n",
    "* x is the value of the random variable (either 0 or 1).\n",
    "* p is the probability of success (value 1).\n",
    "Example of the Bernoulli distribution:\n",
    "Consider flipping a fair coin. Let's say we define \"success\" as getting heads (H) and \"failure\" as getting tails (T). If the coin is fair, the probability of getting heads (success) is p = 0.5, and the probability of getting tails (failure) is also 0.5. In this case, the random variable representing the outcome of the coin flip follows a Bernoulli distribution with p = 0.5.\n",
    "\n",
    "Difference between Bernoulli Distribution and Binomial Distribution:\n",
    "\n",
    "# Number of Trials:\n",
    "\n",
    "* Bernoulli Distribution: Represents a single trial or experiment with only two possible outcomes (success or failure).\n",
    "* Binomial Distribution: Represents the number of successes in a fixed number of independent Bernoulli trials (experiments).\n",
    "# Parameters:\n",
    "\n",
    "* Bernoulli Distribution: Has one parameter, p, which is the probability of success in a single trial.\n",
    "* Binomial Distribution: Has two parameters: n (number of trials) and p (probability of success in a single trial).\n",
    "# Random Variable:\n",
    "\n",
    "* Bernoulli Distribution: The random variable takes values 0 or 1, representing failure or success.\n",
    "* Binomial Distribution: The random variable represents the count of successes in the fixed number of trials and takes integer values from 0 to n.\n",
    "# Formula and PMF:\n",
    "\n",
    "* Bernoulli Distribution: The probability mass function (PMF) of the Bernoulli distribution gives the probabilities of getting 0 or 1 in a single trial.\n",
    "* Binomial Distribution: The probability mass function (PMF) of the binomial distribution gives the probabilities of getting a specific number of successes (k) in n trials.\n",
    "# Example:\n",
    "\n",
    "* Bernoulli Distribution: Coin flips, where you're interested in the probability of getting heads (success) in a single flip.\n",
    "* Binomial Distribution: Counting the number of heads (successes) in a fixed number of coin flips (trials), like getting heads 3 times in 5 coin flips.\n",
    "In summary, the Bernoulli distribution is a special case of the binomial distribution, where there's only one trial. The binomial distribution extends the concept to multiple trials, allowing you to calculate the probabilities of different numbers of successes occurring within those trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56215d5d",
   "metadata": {},
   "source": [
    "# Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greater than 60? Use the appropriate formula and show your calculations.\n",
    "\n",
    "Ans: To find the probability that a randomly selected observation from a normally distributed dataset with a mean of 50 and a standard deviation of 10 will be greater than 60, we need to use the standard normal distribution (also known as the Z-distribution) and the Z-score formula. The Z-score measures how many standard deviations a value is away from the mean.\n",
    "\n",
    "* The Z-score formula is: Z= X−μ/σ\n",
    "\n",
    "Where:\n",
    "\n",
    "* Z is the Z-score.\n",
    "* X is the value we're interested in (in this case, 60).\n",
    "* μ is the mean of the distribution (given as 50).\n",
    "* σ is the standard deviation of the distribution (given as 10).\n",
    "Let's calculate the Z-score for X = 60:\n",
    "* Z= 60−50/10 = 10/10 = 1.\n",
    "\n",
    "Now that we have the Z-score, we can find the probability using a standard normal distribution table or a calculator. The standard normal distribution table gives the probability that a Z-score is less than a given value. Since we want the probability that the observation is greater than 60, we'll subtract the Z-score's corresponding cumulative probability from 1.\n",
    "\n",
    "From a standard normal distribution table, we find that the cumulative probability for Z = 1 is approximately 0.8413.\n",
    "\n",
    "So, the probability that a randomly selected observation will be greater than 60 is:\n",
    "\n",
    "* P(X > 60) = 1 - P(Z < 1) = 1 - 0.8413≈0.1587. \n",
    "\n",
    "Therefore, the probability is approximately 0.1587, or 15.87%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba45ab",
   "metadata": {},
   "source": [
    "# Q7: Explain uniform Distribution with an example.\n",
    "\n",
    "Ans: The uniform distribution is a probability distribution in which all values in a given range are equally likely to occur. In other words, the probability density function (PDF) of a uniform distribution is constant within a specified interval and zero outside that interval.\n",
    "\n",
    "The uniform distribution is often denoted as \"U(a, b),\" where \"a\" is the lower bound of the interval, and \"b\" is the upper bound of the interval.\n",
    "\n",
    "Mathematically, the PDF of a uniform distribution U(a, b) is defined as:\n",
    "\n",
    "f(x) = 1 / (b - a) for a ≤ x ≤ b,\n",
    "f(x) = 0 elsewhere.\n",
    "\n",
    "Here's an example to illustrate the uniform distribution:\n",
    "\n",
    "Suppose we have a fair six-sided die. The outcomes of rolling this die range from 1 to 6, and each outcome is equally likely. This situation can be modeled using a uniform distribution.\n",
    "\n",
    "Let's define the interval [a, b] to represent the possible outcomes of the die roll. In this case, a = 1 (the lowest value on the die) and b = 6 (the highest value on the die).\n",
    "\n",
    "The probability density function (PDF) of the uniform distribution U(1, 6) for this example is:\n",
    "\n",
    "f(x) = 1 / (6 - 1) = 1/5 for 1 ≤ x ≤ 6,\n",
    "f(x) = 0 elsewhere.\n",
    "\n",
    "This means that each value from 1 to 6 on the die has an equal probability of 1/5 of being rolled.\n",
    "\n",
    "To visualize this, imagine a flat, horizontal line segment extending from x = 1 to x = 6 on the x-axis, where the height of the line segment is 1/5 within this interval and 0 outside this interval.\n",
    "\n",
    "This uniform distribution example is straightforward because all outcomes (1, 2, 3, 4, 5, and 6) have the same probability of 1/5, and no other values are possible. In general, the uniform distribution is useful when you have a situation where all values within a specified range are equally likely to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e63cf",
   "metadata": {},
   "source": [
    "# Q8: What is the z score? State the importance of the z score.\n",
    "\n",
    "Ans: The z-score, also known as the standard score, is a statistical measure that quantifies the number of standard deviations a data point is from the mean of a dataset. It's used to standardize data, making it easier to compare different values across different distributions or datasets.\n",
    "\n",
    "Mathematically, the z-score of a data point \"X\" in a dataset with mean \"μ\" and standard deviation \"σ\" is calculated as:\n",
    "\n",
    "* z= X-μ/σ\n",
    "\n",
    "Where:\n",
    "\n",
    "* \"X\" is the data point you want to standardize.\n",
    "* \"μ\" is the mean of the dataset.\n",
    "* \"σ\" is the standard deviation of the dataset.\n",
    "* \"z\" is the z-score.\n",
    "The importance of the z-score:\n",
    "\n",
    "* Standardization: The z-score allows you to standardize data, which is useful when comparing data points from different distributions or datasets. It helps you understand how each data point relates to the overall distribution.\n",
    "\n",
    "* Relative Position: Z-scores give you information about the relative position of a data point within a dataset. A positive z-score indicates that the data point is above the mean, while a negative z-score indicates that it's below the mean.\n",
    "\n",
    "* Outlier Detection: Z-scores can be used to identify outliers in a dataset. Data points with z-scores significantly larger or smaller than a certain threshold (usually around ±2 or ±3) may be considered outliers.\n",
    "\n",
    "* Probability: The z-score can be used to calculate the probability of finding a value within a certain range in a normal distribution. This is particularly useful for hypothesis testing and confidence intervals.\n",
    "\n",
    "* Comparisons: Z-scores allow you to compare different data points in terms of their distance from the mean, regardless of the original units of measurement. This is especially valuable when dealing with data measured in different scales or units.\n",
    "\n",
    "* Data Transformation: Z-scores are commonly used in data preprocessing for machine learning, especially when features (variables) have different units or scales. Standardizing features to have a mean of 0 and a standard deviation of 1 can improve the performance of certain algorithms.\n",
    "\n",
    "Overall, the z-score is a powerful tool in statistics that helps in standardizing, comparing, and analyzing data. Its importance spans various fields, including statistical analysis, data science, hypothesis testing, and outlier detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b9f0e",
   "metadata": {},
   "source": [
    "# Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem.\n",
    "\n",
    "Ans:The Central Limit Theorem (CLT) is a fundamental concept in statistics that describes the behavior of the sum (or average) of a large number of independent and identically distributed random variables. It states that under certain conditions, the distribution of the sum (or average) of these variables will approximate a normal distribution, regardless of the original distribution of the individual variables.\n",
    "\n",
    "The Central Limit Theorem is crucial for statistical inference and hypothesis testing, especially when dealing with large sample sizes. It has the following key characteristics:\n",
    "\n",
    "* Normal Distribution: When the sample size is sufficiently large (usually considered to be greater than 30), the distribution of the sample mean will be approximately normal, even if the original population distribution is not normal. This allows us to apply methods that assume a normal distribution, such as calculating confidence intervals and conducting hypothesis tests.\n",
    "\n",
    "* Stability: The Central Limit Theorem holds regardless of the shape of the original population distribution, as long as the individual observations are independent and the sample size is large enough.\n",
    "\n",
    "* Sampling Variability: The Central Limit Theorem explains why the distribution of sample means becomes narrower as the sample size increases. This reduction in variability is a result of averaging a larger number of observations, which reduces the effect of individual outliers or extreme values.\n",
    "\n",
    "The significance of the Central Limit Theorem:\n",
    "\n",
    "* Statistical Inference: The CLT allows us to make inferences about population parameters based on sample data. For example, we can estimate the population mean and calculate confidence intervals for it, even if we don't know the original distribution of the population.\n",
    "\n",
    "* Hypothesis Testing: The CLT is crucial for hypothesis testing, where we compare sample statistics (like the sample mean) to hypothesized population parameters. The assumption of a normal distribution for the sample mean allows us to use standard normal distribution properties for hypothesis tests.\n",
    "\n",
    "* Real-world Applications: Many real-world phenomena can be modeled using the normal distribution, and the Central Limit Theorem enables us to use the normal distribution in situations where the sample size is sufficiently large, even if the original data does not follow a normal distribution.\n",
    "\n",
    "* Stabilizes Estimates: The Central Limit Theorem stabilizes sample estimates as the sample size increases. This means that larger sample sizes lead to more reliable and stable estimates of population parameters.\n",
    "\n",
    "Overall, the Central Limit Theorem is a fundamental concept that underpins many statistical methods, making it a cornerstone of statistical theory and practice. It allows us to use the powerful tools of the normal distribution in situations where we often encounter sample data from a wide range of distributions.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f11827",
   "metadata": {},
   "source": [
    "# Q10: State the assumptions of the Central Limit Theorem.\n",
    "\n",
    "Ans: The Central Limit Theorem (CLT) is a powerful statistical concept that provides important insights into the behavior of sample means or sums. However, it relies on certain assumptions to hold. The key assumptions of the Central Limit Theorem are as follows:\n",
    "\n",
    "* Independence: The individual observations or measurements within the sample should be independent of each other. This means that the value of one observation should not influence the value of another observation. Independence is essential because the CLT relies on the effects of random variability across the independent observations.\n",
    "\n",
    "* Identically Distributed: The individual observations in the sample should be drawn from the same probability distribution. This means that each observation should have the same mean (μ) and the same standard deviation (σ) as all other observations in the sample. This assumption ensures that the sample statistics accurately represent the population parameters.\n",
    "\n",
    "* Sample Size: The sample size should be sufficiently large. While there is no strict rule for what constitutes a \"sufficiently large\" sample size, a common guideline is that the sample size should be greater than or equal to 30. In practice, larger sample sizes often lead to better approximations to a normal distribution.\n",
    "\n",
    "* No Extreme Outliers: The presence of extreme outliers or highly skewed data can potentially violate the assumptions of the Central Limit Theorem. The CLT assumes that there are no extreme deviations from the mean that would significantly affect the behavior of the sample mean.\n",
    "\n",
    "* Finite Variance: The population from which the samples are drawn should have a finite variance. This ensures that the sample mean and sample sum have finite variances as well. If the variance is infinite or undefined, the assumptions of the CLT may not hold.\n",
    "\n",
    "It's important to note that while the Central Limit Theorem provides valuable insights into the sampling distribution of the sample mean or sum, it is not universally applicable. Violation of any of these assumptions may limit the accuracy of the normal distribution approximation. In cases where the assumptions of the CLT are not met, alternative statistical methods or distributions may need to be considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fde8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
